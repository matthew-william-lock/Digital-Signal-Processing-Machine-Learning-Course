# Back Propagation

This involves finding the gradient for each weights and applying gradient descent
- Allows us to calculate error from froward pass and determining gradients via chain rule

<br> <img src="https://latex.codecogs.com/gif.latex?\frac{\text{d}}{\text{d}y}[f(g(x))]=\frac{\text{d}f}{\text{d}g}\frac{\text{d}g}{\text{d}x}"/> <br>
<br> <img src="https://latex.codecogs.com/gif.latex?\frac{\text{d}}{\text{d}y}[f(g(x)),h(x)]=\frac{\text{d}f}{\text{d}g}\frac{\text{d}g}{\text{d}x}+\frac{\text{d}f}{\text{d}h}\frac{\text{d}}{\text{d}x}"/> <br>

- Weights propagate through all connected neurons and finally towards the output value
- It seems that adding more layers will add more complexity

<a name="backward_pass">![../Images/backward_pass.png](../Images/backward_pass.png)

## Simplification

<a name="backward_pass_1">![../Images/backward_pass_1.png](../Images/backward_pass_1.png)

## Backward Propagation Equations

<a name="backward_pass_2">![../Images/backward_pass_2.png](../Images/backward_pass_2.png)

## Update Parameters

<a name="grad_desc_rules">![../Images/grad_desc_rules.png](../Images/grad_desc_rules.png)
