# Regularization 
*General idea is to constrain the parameters so that they tend towards zero, results in less complex models*

- Constraint parameters
    - Small Values
    - Reduced complexity
- Embedded in the algorithms

<br> <img src="https://latex.codecogs.com/gif.latex?y=0.1x_1+0.01x_2"/> <br>
- Small value of x2 results in feature vanishing

## l1 Regularization
- For linear models : LASSO (Least Absolute Shrinkage and Selection Operator) Regression
- l1 penalty: <img src="https://latex.codecogs.com/gif.latex?||\theta||_1=\sum_{j=1}^m|\theta_j|"/>
- Sum of the absolute values of each of the parameters
- Add to objective function
- <img src="https://latex.codecogs.com/gif.latex?J(\theta)=MSA+\lambda\sum_{j=1}^m|\theta_j|"/>
- Goal is to minimize objective function
- Additional term here creates a boundary

<a name="l1reg">![Images/l1reg.png](Images/l1reg.png)

## l2 Regularization
- For linear models: Ridge Regression
- l2 penalty:<img src="https://latex.codecogs.com/gif.latex?||\theta||_2=\sqrt{\sum_{j=1}^m\theta_j^2}"/>
- Add to objective function
- <img src="https://latex.codecogs.com/gif.latex?J(\theta)=MSA+(||\theta||)^2=MSE+\lambda\sum_{j=1}^m\theta_j^2"/>
- Minimizing MSE under a constraint
- Small values of lambda cause the circle to increase
- The larger the value of lambda, the smaller the circle. This leads to coefficients closers to zero and a less complex model
- Intersection point is less likely to happen to at intersects, we therefore are able to keep some model complexity as parameters are not completely eliminated

<a name="l2reg">![Images/l2reg.png](Images/l2reg.png)