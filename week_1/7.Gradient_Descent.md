# Gradient Descent

- We can update our parameters according to the following rule:
<br>.<img src="https://latex.codecogs.com/gif.latex?\theta_j:=\theta_j-\eta\frac{\partial{J(\theta)}}{\partial{\theta_j}}" /> <br>
- <img src="https://latex.codecogs.com/gif.latex?\theta_j:" /> is the jth parameter
- ':=' is an assignment operator
- <img src="https://latex.codecogs.com/gif.latex?\eta" /> is the learning rate
- <img src="https://latex.codecogs.com/gif.latex?\frac{\partial{J(\theta)}}{\partial{\theta_j}}" /> is the gradient of the objective function with respect to <img src="https://latex.codecogs.com/gif.latex?\theta_j:" />

## How this works
1. Value of theta moves in a direction opposite to the gradient, proportional to the learning rate
2. If the gradient is zero, we have found a local minimum and theta will no longer move

## Calculating the Gradients
<a name="gradients">![Images/gradients.png](Images/gradients.png)