# Simple Linear Regression
*This is a supervised learning algorithm*

- Assumes a linear model
- One output response value and single input feature
<br><img src="https://latex.codecogs.com/gif.latex?h_\theta(x)=y\approx\theta_0+\theta_1x" /> <br>
<br>E.g.<img src="https://latex.codecogs.com/gif.latex?battery-life=y\approx\theta_0+\theta_1\timesscreen-size" /> <br>

# Optimization

- Chose optimization metric (MSE)
- Chose optimization algorithm
    - Ordinary Least Squares (analytical solution)
        - Analytical solutions are not always easy to determine
    - Gradient Descent (numerical solution)
        - Intuitive approach

*Optimization Metric (MSE):*
<br>E.g.<img src="https://latex.codecogs.com/gif.latex?J(\theta_0,\theta_1)=\frac{1}{N}\displaystyle\sum\limits_{i=1}^N(h_\theta(x_i)-y_i^2)" /> <br>
<br>E.g.<img src="https://latex.codecogs.com/gif.latex?=\frac{1}{N}\displaystyle\sum\limits_{i=1}^N(\theta_0+\theta_1x_i)-y_i^2)" /> <br>
<br>E.g.<img src="https://latex.codecogs.com/gif.latex?=\frac{1}{N}\displaystyle\sum\limits_{i=1}^N(\hat{y_i}-y_i^2)" /> <br>

- In terms of theta 0 and theta 1 (parameters)
- Summation of hypothesis function output minus the true output
- Square that and take the average over all of these samples 