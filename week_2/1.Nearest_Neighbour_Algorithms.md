# Nearest Neighbor Algorithms

## Supervised Learning
1. **Classification**
2. Regression - simple and easy to implement. Presumption that data fits linear model is limiting.

**parametric model** = types of algorithm that makes assumptions of form of the solution


## Classification

*We will be focusing on nearest k neighbor algorithm (KNN)*:
- Lazy approach to learning
- Training data used during testing (similar to open boot test)
- Alternative is **eager** learning - training data not used during testing

<a name="knn_example1">![Images/knn_example1.png](Images/knn_example1.png)

The above image shows an example of knn. For knn we need:
- Distance metric to find how similar input vectors are. This can be simple euclidean metric
- k nearest neighbors for a point
- Classify new point with neighbors which it is most similar to

**The Scenario**
<br>
Suppose we have some target function <img src="https://latex.codecogs.com/gif.latex?f(x)=y"/> <br>
where <img src="https://latex.codecogs.com/gif.latex?y\epsilon\lbrace{1,..,t}\rbrace"/> and <img src="https://latex.codecogs.com/gif.latex?t"/> is the number of classes <br>
<img src="https://latex.codecogs.com/gif.latex?f:R^m\rightarrow\lbrace1,...,t\rbrace"/> where m is the number of features (dimensionality).

Training set 
- <img src="https://latex.codecogs.com/gif.latex?D=\lbrace(x^{[i]},y^{[i]},...(x^{[n]},y^{[n]})\rbrace" />
- these are input and label pairs
K-Nearest Neighbors
- <img src="https://latex.codecogs.com/gif.latex?D_k=\lbrace(x^{[i]},y^{[i]},...(x^{[k]},y^{[k]})\rbrace" />
- need to know the distance metric to determine distances
Classification
- <img src="https://latex.codecogs.com/gif.latex?h(x^{[t]})=mode(\{y^{[i]},...,y^{[k]}\})" />
Regression
- <img src="https://latex.codecogs.com/gif.latex?h(x^{[t]})=\frac{1}{k}\displaystyle\sum\limits_{i=1}^ky^{[i]}" />

## Pros and Cons
- Easy to implement
- No assumptions about the data
- Limited by memory constraints
- Computationally complex
    - O(n x m)
    - n = # number of training examples
    - m = # of features
    - Assuming n>>m O(n)
- Curse of Dimensionality 